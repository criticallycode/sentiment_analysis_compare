{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Sentiment Analysis Module\n",
    "\n",
    "Sentiment analysis is the utilization of natural language processing techniques to understand the \"sentiment\" or attutide/tone of a body of text. Sentiment analysis proves useful for tasks like analysis of product reviews, employee feedback, and trending topics and reactions.\n",
    "\n",
    "This project creates a customized sentiment analysis module that combines together the classifications of many different classification algorithms. This ensemble method creates a classifier which votes on the sentiment of text pieces. \n",
    "\n",
    "The approach for designing this ensemble classifier will be as follows:\n",
    "\n",
    "We'll prep the data for use in training our classifier, doing things like removing stop words, etc. Next we'll turn the words into features for our classifier, creating feature sets - along with their associated labels. Next, we'll train our chosen classifiers on the feature sets, and then \"Pickle\" the trained classifier so they don't need to be retrained.\n",
    "\n",
    "After all the classifiers have been trained, we'll create a custom class that combines the decision of all classifiers and renders a judgement. \n",
    "\n",
    "After that, we'll load the pickled weights into a new file and then create a function to return the combined vote of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we should do is import all the libraries we will need to create the ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we'll need to load in the data we wish to use for the training of our classifiers. The data is two text files, one containing examples of positive sentiment reviews while the other contains negative sentiment reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = open(\"positive.txt\",\"r\", encoding='utf-8', errors='replace').read()\n",
    "neg_text = open(\"negative.txt\",\"r\", encoding='utf-8', errors='replace').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to need lists to store the data from our files in. We'll create a list for all the words and a list for the individual reviews in our documents. Each line in the document is a different review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "reviews = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the NLTK library to filter all the words in our two datasets. We're only interested in certain parts of speech, so we'll specify which parts of speech should be used to filter the words. We'll also need to select some stop words as we don't want extremely common words to be analyzed. Thankfully, NLTK has a set of stop words built in. The allowed word types refers to the way NLTK tags different parts of speech. We're interested mainly in adjectives (J), adverbs (R) and verbs (V). If we wanted, we could also throw in nouns (N), but it probably wouldn't be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_word_types = [\"J\", \"R\", \"V\"]\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to need to use the parts of speech we've selected to make a list of all the words we want in our training corpus. We're going to append a label to every review/line in the training docs, so we'll split on lines and add \"Pos\" to the positives and \"Neg\" for negatives. Then, we need to represent our words as numbers our algorithms can interpret, or \"Tokenize\" them for our algorithms. We'll use the `word_tokenize` function in NLTK for this. Finally, for all the words that match our parts of speech, assuming the words aren't in our list of stop words, we'll add them to the list of all words we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pos_text.split('\\n'):\n",
    "    # Note that we take all the words in the doc and append a label to them\n",
    "    reviews.append((p, \"pos\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            if w[0] not in stop_words:\n",
    "                all_words.append(w[0].lower())\n",
    "\n",
    "for p in neg_text.split('\\n'):\n",
    "    reviews.append((p, \"neg\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            if w[0] not in stop_words:\n",
    "                all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to transform the lists of words/tokens into a list of features we want to use for training. We probably don't want to use all the words for training, as this would take quite a long time. Instead, let's just select the top 5000 words. In order to get the top 5000 words, we want to get the word count of all the words and then grab the top 5000. It's important to be aware of how the frequency distribution is returned. It's basically returned as a dictionary with key-value pairs, and we'll be using the keys later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'s\", 1709), (\"n't\", 940), ('much', 386), ('even', 382), ('good', 370), ('little', 302), ('make', 273), ('never', 262), ('enough', 260), ('funny', 255), ('makes', 252), ('bad', 234), ('best', 232), ('new', 206), ('really', 197), ('well', 196), ('made', 193), ('many', 183), ('still', 179), ('see', 177)]\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# frequency distribution gives words in order of most common to least common, essentially a key:val pair with\n",
    "# a frequency val for every word(key)\n",
    "word_dist = nltk.FreqDist(all_words)\n",
    "print(word_dist.most_common(20))\n",
    "\n",
    "word_features = list(word_dist.keys())[:5000]\n",
    "print(len(word_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pickle the words and the features, as these features as what our classifier will use to reason about future text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_review_files = open(\"pickled_docs.pickle\", \"wb\")\n",
    "pickle.dump(reviews, save_review_files)\n",
    "save_review_files.close()\n",
    "\n",
    "save_features = open(\"pickled_features5k.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_features)\n",
    "save_features.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the features we want to use to classify a document, but we now have to create a function to extract the features from a document we want to classify. Here's where it's important to remember that our word features exist as a key-value pair. We want to extract the keys from the document we're classifying. After we tokenize the document, we'll check the document to see if the keys are in it. After we get the keys, we put them in a list of features and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(document):\n",
    "    # The words are the first part of the set, with the occurence rate being the second part\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    # for every word in the list of word features (the words we care about)\n",
    "    # the key in the feature's dictionary must be equal to boolean value of w in words\n",
    "    # if the word in the dictionary is in the set of document (is within the document at all)\n",
    "    # a True value is returned\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can get the desired features out of the document. However, we'll also need the labels for these features. We can use the function we just created to collect the features from the documents, and then get the label from our list of reviews. We also need to shuffle up the data, because as it exists now the training data would be all positive and then all negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sets = [(feature_extractor(review), category) for (review, category) in reviews]\n",
    "random.shuffle(features_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, we should pickle the feature sets now that we have our feature/label pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features_labels = open(\"pickled_features_labels_5k.pickle\",\"wb\")\n",
    "pickle.dump(features_sets, save_features_labels)\n",
    "save_features_labels.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both our featues and labels contained in a variable list, we can split the list up into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = features_sets[:10000]\n",
    "testing_data = features_sets[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose some classifiers to use. In this instance, we want an odd number of classifiers since there will be a vote and we want a tie-breaker. We'll be trying Naive Bayes, along with Multinomal Naive Bayes and Bernoulli Naive Bayes. We'll also use:\n",
    "the Logistic Regression classifier, the Linear Support Vector Machine classifer, NuSVC, Stochastic Gradient Descent, K-Nearest Neighbors, and the Decision Tree Classifier. \n",
    "\n",
    "Let's see which ones perform best and then schoose some to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_clf = nltk.NaiveBayesClassifier.train(testing_data)\n",
    "MNB_clf = SklearnClassifier(MultinomialNB())\n",
    "BNB_clf = SklearnClassifier(BernoulliNB())\n",
    "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
    "SGDC_clf= SklearnClassifier(SGDClassifier())\n",
    "LinSVC_clf = SklearnClassifier(LinearSVC())\n",
    "NuSVC_clf = SklearnClassifier(NuSVC())\n",
    "KNN_clf = SklearnClassifier(KNeighborsClassifier(n_neighbors=3))\n",
    "DT_clf = SklearnClassifier(DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the classifiers. Note that this is done differently than you would normally do in Scikit-learn. Since we are using versions of the classifier from NLTK we call `train` on them, rather than fit and predicting. Please note that this can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: <nltk.classify.naivebayes.NaiveBayesClassifier object at 0x000002987FCC0588>\n",
      "Training: <SklearnClassifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))>\n",
      "Training: <SklearnClassifier(BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))>\n",
      "Training: <SklearnClassifier(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: <SklearnClassifier(SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False))>\n",
      "Training: <SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))>\n",
      "Training: <SklearnClassifier(NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
      "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "      kernel='rbf', max_iter=-1, nu=0.5, probability=False, random_state=None,\n",
      "      shrinking=True, tol=0.001, verbose=False))>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\daniel\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: <SklearnClassifier(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "                     weights='uniform'))>\n",
      "Training: <SklearnClassifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best'))>\n"
     ]
    }
   ],
   "source": [
    "clf_list = [NB_clf, MNB_clf, BNB_clf, LogReg_clf, SGDC_clf, LinSVC_clf, NuSVC_clf, KNN_clf, DT_clf]\n",
    "\n",
    "for clf in clf_list:\n",
    "    classifier = str(clf)\n",
    "    print(\"Training: \" + classifier)\n",
    "    clf.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that they have trained we can test the classifiers on the validation set and see what their accuracy. We'll use a dictionary to contain the name and the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Naive Bayes accuracy is:\n",
      "93.22289156626506\n",
      "Multinomial Naive Bayes accuracy is:\n",
      "73.19277108433735\n",
      "Bernoulli Naive Bayes accuracy is:\n",
      "72.28915662650603\n",
      "Logistic Regression accuracy is:\n",
      "71.98795180722891\n",
      "SGDC accuracy is:\n",
      "68.22289156626506\n",
      "Linear SVC accuracy is:\n",
      "69.7289156626506\n",
      "Nu SVC accuracy is:\n",
      "69.87951807228916\n",
      "Decision Tree accuracy is:\n",
      "64.7590361445783\n",
      "K-Nearest Neighbors accuracy is:\n",
      "55.42168674698795\n"
     ]
    }
   ],
   "source": [
    "clf_names = {'Vanilla Naive Bayes': NB_clf,\n",
    "            'Multinomial Naive Bayes': MNB_clf,\n",
    "            'Bernoulli Naive Bayes': BNB_clf,\n",
    "            'Logistic Regression': LogReg_clf,\n",
    "            'SGDC': SGDC_clf,\n",
    "            'Linear SVC': LinSVC_clf,\n",
    "            'Nu SVC': NuSVC_clf,\n",
    "            'Decision Tree': DT_clf,\n",
    "            'K-Nearest Neighbors': KNN_clf}\n",
    "\n",
    "for key, val in clf_names.items():\n",
    "        print(key + \" \" + \"accuracy is:\")\n",
    "        print(nltk.classify.accuracy(val, testing_data) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the accuracy that I received for the classifiers (this tends to fluctuate a bit, so you might get something different):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla Naive Bayes accuracy is:\n",
    "93.97590361445783\n",
    "\n",
    "Multinomial Naive Bayes accuracy is:\n",
    "72.43975903614458\n",
    "\n",
    "Bernoulli Naive Bayes accuracy is:\n",
    "71.3855421686747\n",
    "\n",
    "Logistic Regression accuracy is:\n",
    "71.83734939759037\n",
    "\n",
    "SGDC accuracy is:\n",
    "70.03012048192771\n",
    "\n",
    "Linear SVC accuracy is:\n",
    "70.63253012048193\n",
    "\n",
    "Nu SVC accuracy is:\n",
    "71.98795180722891\n",
    "\n",
    "Decision Tree accuracy is:\n",
    "65.06024096385542\n",
    "\n",
    "K-Nearest Neighbors accuracy is:\n",
    "55.12048192771084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick only the classifiers that performed the best. It looks like K-Nearest Neighbors performed barely better than chance, so let's drop it. Regular Naive Bayes performs extremely well, in fact it performed suspiciously well. It seems likely that the algorithm could be overfitting. For that reason, let us drop it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the classifiers have been trained and the classifiers we want to use chosen, we'll want to pickle them as well so that we don't have to retrain them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_BNB_classifier = open(\"BNBclf5k.pickle\",\"wb\")\n",
    "pickle.dump(BNB_clf, save_BNB_classifier)\n",
    "save_BNB_classifier.close()\n",
    "\n",
    "save_MNB_classifier = open(\"multinaivebayes5k.pickle\",\"wb\")\n",
    "pickle.dump(MNB_clf, save_MNB_classifier)\n",
    "save_MNB_classifier.close()\n",
    "\n",
    "save_LogReg_classifier = open(\"LogReg5k.pickle\",\"wb\")\n",
    "pickle.dump(LogReg_clf, save_LogReg_classifier)\n",
    "save_LogReg_classifier.close()\n",
    "\n",
    "save_SGDC_classifier = open(\"SGDC5k.pickle\",\"wb\")\n",
    "pickle.dump(SGDC_clf, save_SGDC_classifier)\n",
    "save_SGDC_classifier.close()\n",
    "\n",
    "save_LinSVC_classifier = open(\"LinSVC5k.pickle\",\"wb\")\n",
    "pickle.dump(LinSVC_clf, save_LinSVC_classifier)\n",
    "save_LinSVC_classifier.close()\n",
    "\n",
    "save_NuSVC_classifier = open(\"NuSVC5k.pickle\",\"wb\")\n",
    "pickle.dump(NuSVC_clf, save_NuSVC_classifier)\n",
    "save_NuSVC_classifier.close()\n",
    "\n",
    "save_DT_classifier = open(\"DT5k.pickle\",\"wb\")\n",
    "pickle.dump(DT_clf, save_DT_classifier)\n",
    "save_DT_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features, labels, and classifier have been set up. This means we just need to create the voting classifier now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifier(ClassifierI):\n",
    "\n",
    "    def __init__(self, *classifiers):\n",
    "        # The classifiers of this class are the classifiers we've specified above\n",
    "        self.__classifiers = classifiers\n",
    "\n",
    "    # This function overrides the default classifier in that originates from NLTK\n",
    "    def classify(self, features):\n",
    "        # Need a way to store the votes from the individual classifiers\n",
    "        votes = []\n",
    "\n",
    "        # For all the classifiers, classify the features\n",
    "        # append the result of the classification to the votes list\n",
    "        for i in self.__classifiers:\n",
    "            v = i.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        # The classification will be the mode of all the votes\n",
    "        return mode(votes)\n",
    "\n",
    "    # We may also want to include a confidence statistic - \n",
    "    # which reflects how many classifiers voted in favor of the class\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self.__classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        # instead of the pure mode, we count how many classifiers voted for the mode\n",
    "        # and then divide by the number of votes\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the accuracy of our voting classifier. This initial test will take a while, though when we use it to classfify individual pieces of text, it should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier test:\n",
      "Voted Classifier accuracy: 70.93373493975903\n"
     ]
    }
   ],
   "source": [
    "print(\"Voting classifier test:\")\n",
    "\n",
    "vote_clf = VotingClassifier(MNB_clf, BNB_clf, LogReg_clf, LinSVC_clf, NuSVC_clf, SGDC_clf, DT_clf)\n",
    "print(\"Voted Classifier accuracy:\", (nltk.classify.accuracy(vote_clf, testing_data)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what I got when I ran this:\n",
    "\n",
    "Voting classifier test:\n",
    "Voted Classifier accuracy: 72.13855421686746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we have about 72% accuracy, but this seems somewhat volatile as during testing it went as high as 74 or 75%. In general, over multiple tests, it seems to fluctuate between 69% to 74%. It does seem to be performing at least as well or better than most of our classifiers, and does much better than our weakest classifiers. A voting classifier/ensemble method should be more robust to overfitting as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we print a couple example sentences to see if it is working like we intend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: neg Confidence: %: 1.0\n",
      "Classification: neg Confidence: %: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check performance on sample reviews\n",
    "print(\"Classification:\", vote_clf.classify(testing_data[0][0]),\n",
    "      \"Confidence: %:\", vote_clf.confidence(testing_data[0][0]))\n",
    "\n",
    "print(\"Classification:\", vote_clf.classify(testing_data[1][0]),\n",
    "      \"Confidence: %:\", vote_clf.confidence(testing_data[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our Voting Classifier we now need to load the pickled data back in (if we're running this outside of a notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_file = open(\"pickled_docs.pickle\", \"rb\")\n",
    "documents = pickle.load(documents_file)\n",
    "documents_file.close()\n",
    "\n",
    "word_features_file = open(\"pickled_features5k.pickle\", \"rb\")\n",
    "word_features = pickle.load(word_features_file)\n",
    "word_features_file.close()\n",
    "\n",
    "feature_sets_file = open(\"pickled_features_labels_5k.pickle\", \"rb\")\n",
    "feature_sets = pickle.load(feature_sets_file)\n",
    "feature_sets_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is create a function to classify inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_clf = VotingClassifier(MNB_clf, BNB_clf, LogReg_clf, LinSVC_clf, NuSVC_clf, SGDC_clf, DT_clf)\n",
    "\n",
    "# We can just import this function into another script to use our custom classifier\n",
    "\n",
    "def sentiment(text):\n",
    "    feats = feature_extractor(text)\n",
    "    return vote_clf.classify(feats), vote_clf.confidence(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we give it a shot on some custom data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 1.0)\n",
      "('pos', 0.7142857142857143)\n"
     ]
    }
   ],
   "source": [
    "text = \"This game is terrible. The controls are garbage and so are the character models, I hate everything about it.\"\n",
    "text2 = \"I love her so much. She makes me really happy.\"\n",
    "\n",
    "analyzer = sentiment(text)\n",
    "analyzer2 = sentiment(text2)\n",
    "\n",
    "print(analyzer)\n",
    "print(analyzer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how our custom module is performing, let's compare it to some other sentiment analysis modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence classification - Test sentence is :  This game is terrible. The controls are garbage and so are the character models, I hate everything about it.\n",
      "---\n",
      "TextBlob: -0.7333333333333334\n",
      "Vader: {'neg': 0.298, 'neu': 0.702, 'pos': 0.0, 'compound': -0.7783}\n",
      "Custom: ('neg', 1.0)\n",
      "\n",
      "Sentence classification - Test sentence is :  I love her so much. She makes me really happy.\n",
      "---\n",
      "TextBlob: 0.5\n",
      "Vader: {'neg': 0.0, 'neu': 0.461, 'pos': 0.539, 'compound': 0.8479}\n",
      "Custom: ('pos', 0.7142857142857143)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sentences = [text, text2]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for s in sentences:\n",
    "    print(\"Sentence classification - \" + \"Test sentence is :  \" + str(s))\n",
    "    print(\"---\")\n",
    "    text_blob = TextBlob(s)\n",
    "    # For Textblob, range runs from [-1.0, 1.0], Above 0 is positive\n",
    "    print(\"TextBlob: \" + str(text_blob.sentiment.polarity))\n",
    "    v_sent = analyzer.polarity_scores(s)\n",
    "    print(\"Vader: \" + str(v_sent))\n",
    "    custom = sentiment(s)\n",
    "    print(\"Custom: \" + str(custom))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did they compare? It looks like our classifier is more sure about the two examples than the other classifiers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
